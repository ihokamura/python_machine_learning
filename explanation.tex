\documentclass[uplatex]{jsarticle}
\usepackage{amsmath, amssymb, amsthm, bm, url}

\theoremstyle{definition}
\newtheorem{definition}{定義}[section]
\newtheorem{algorithm}[definition]{アルゴリズム}
\newtheorem{corollary}[definition]{系}
\newtheorem{lemma}[definition]{補題}
\newtheorem{proposition}[definition]{命題}
\newtheorem{theorem}[definition]{定理}

\numberwithin{equation}{section}

\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}

\renewcommand{\d}{\mathrm{d}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\proofname}{\bf{証明}}
\renewcommand{\labelenumi}{(\theenumi)}

\begin{document}
\section{分類問題ー単純な機械学習アルゴリズムのトレーニング}
与えられた入力をクラス$1$(陽性クラス)とクラス$-1$(陰性クラス)に分類する二値分類タスクを考える.
トレーニングデータとして, 入力値$\bm{x}^{(i)} \in \R^{m}$と対応するクラス$y^{(i)} \in \{-1, 1\}$ $(i = 1, \dots, n)$が与えられているとする.
決定関数$\phi$は総入力$z$, すなわち, 入力値$\bm{x}$と対応する重み$\bm{w}$の線型結合
\begin{equation}
    z = \bm{w}^{T}\bm{x} = \sum_{j = 1}^{m} w_{j}x_{j}
\end{equation}
を引数として受け取る.

\subsection{パーセプトロン}
パーセプトロンでは, サンプル$\bm{x}^{(i)}$に対する総入力が
指定された閾値$\theta$より大きい場合はクラス1を予測し, 
そうでない場合はクラス$-1$を予測する.
つまり, 決定関数$\phi$として階段関数
\begin{equation}
    \phi(z) = 
    \begin{cases}
        1  & (z \geq \theta) \\
        -1 & (z < \theta)
    \end{cases}    
\end{equation}
を採用する.
ここで, 数式を単純にするため, 閾値$\theta$を右辺に移動し, 
添字$0$の入力を$x_{0} = 1$, 重みを$w_{0} = -\theta$と定義する.
このとき, 総入力と決定関数はそれぞれ
\begin{equation}
    z = \bm{w}^{T}\bm{x} = \sum_{j = 0}^{m} w_{j}x_{j}
\end{equation}
\begin{equation}
    \phi(z) = 
    \begin{cases}
        1  & (z \geq 0) \\
        -1 & (z < 0)
    \end{cases}    
\end{equation}
と表される.
機械学習の分野では, 負の重み$w_{0} = -\theta$をバイアスユニットと呼ぶ.
パーセプトロンの学習アルゴリズムは以下の通りである.
\begin{algorithm}
    (パーセプトロン)
    \begin{enumerate}
        \item
        重み$w_{j}$を$0$または小さい乱数で初期化する.
        \item
        各トレーニングサンプル$\bm{x}^{(i)}$に対して以下を実行する.
            \begin{enumerate}
                \item
                出力値$\hat{y}^{(i)}$を
                \begin{equation}
                    \hat{y}^{(i)} = \phi(\bm{w}^{T}\bm{x}^{(i)})
                \end{equation}
                により計算する.
                \item
                重み$w_{j}$を
                \begin{equation}
                    \begin{cases}
                        w_{j} \leftarrow w_{j} + \Delta w_{j} \\
                        \Delta w_{j} = \eta(y^{(i)} - \hat{y}^{(i)})x^{(i)}_{j}
                    \end{cases}
                \end{equation}
                により更新する.
                ただし, $0 < \eta \leq 1$は学習率である.
            \end{enumerate}
    \end{enumerate}
\end{algorithm}
パーセプトロンの収束が保証されるのは, 2つのクラスが線形分離可能で, 学習率が十分に小さい場合に限られる.
2つのクラスが線形分離可能でない場合, データセットに対するトレーニングの最大回数(エポック)や誤分類の最大数を設定する必要がある.

\subsection{ADALINE}
ADALINE(ADAptive LInear NEuron)では, 重みの更新において階段関数ではなく線型関数を利用する.
すなわち, 線型関数$\phi$を用いてコスト関数$J$を
\begin{equation}
    J(\bm{w}) = \frac{1}{2}\sum_{i = 1}^{n}(y^{(i)} - \phi(\bm{w}^{T}\bm{x}^{(i)}))^{2}
\end{equation}
で定義し, これを最小化する重み$\bm{w}$を探索する.
その後, 最終的な予測はパーセプトロンと同様に階段関数で決定される.
重みの更新は勾配降下法に基づいて行われる.
\begin{algorithm}
    (ADALINE)
    \begin{enumerate}
        \item
        重み$\bm{w}$を小さい乱数で初期化する.
        \item
        重み$\bm{w}$を
        \begin{equation}
            \begin{cases}
                \bm{w} \leftarrow \bm{w} + \Delta\bm{w} \\
                \Delta\bm{w} = -\eta\nabla J(\bm{w})
            \end{cases}
        \end{equation}
        により更新する.
        ただし, $0 < \eta \leq 1$は学習率である.
    \end{enumerate}
\end{algorithm}
線型活性化関数を$\phi(z) = az$とすると, 重みの更新式は
\begin{equation}
    \Delta w_{j} = a\eta\sum_{i = 1}^{n} (y^{(i)} - \phi(\bm{w}^{T}\bm{x}^{(i)}))x^{(i)}_{j}
\end{equation}
で与えられる.

このように, トレーニングデータの全サンプルに基づいて行われる勾配降下法をバッチ勾配降下法と呼ぶ.
重みが早く収束するような学習率$\eta$を見つけるには, 一般にはある程度の実験が必要である.
\begin{itemize}
    \item
    学習率が大きすぎると, エポックごとにコスト関数が増大していく可能性がある.
    \item
    学習率が小さすぎると, 重みが収束するまでのエポック数が大きくなる可能性がある.
\end{itemize}

勾配降下法の収束が早めるためには, 特徴量をスケーリングすることが有効である.
特に, トレーニングデータを標準化するとコスト関数の条件数が1に近づくため, エポック数を抑えることができる.

トレーニングデータが大規模になると, それら全体から重みを更新するバッチ勾配降下法は計算コストが高い.
この場合, トレーニングサンプルごとに重みを更新するオンライン勾配降下法(確率的勾配降下法)が有効である.
\begin{algorithm}
    (オンライン勾配降下法)
    \begin{enumerate}
        \item
        重み$\bm{w}$を小さい乱数で初期化する.
        \item
        各トレーニングサンプル$\bm{x}^{(i)}$に対して, 重み$\bm{w}$を
        \begin{equation}
            \begin{cases}
                \bm{w} \leftarrow \bm{w} + \Delta\bm{w} \\
                \Delta\bm{w} = a\eta(y^{(i)} - \phi(\bm{w}^{T}\bm{x}^{(i)}))\bm{x}^{(i)}
            \end{cases}
        \end{equation}
        により更新する.
        ただし, $0 < \eta \leq 1$は学習率である.
    \end{enumerate}
\end{algorithm}
バッチ勾配降下法とは異なり, オンライン勾配降下法では総入力$z$を計算するときの重み$\bm{w}$がトレーニングサンプルごとに変化することに注意する.


\section{分類問題ー機械学習ライブラリscikit-learnの活用}
\subsection{ロジスティック回帰}
与えられた入力をクラス$1$(陽性クラス)とクラス$0$(陰性クラス)に分類する二値分類タスクを考える.
トレーニングデータとして, 入力値$\bm{x}^{(i)} \in \R^{m}$と対応するクラス$y^{(i)} \in \{-1, 1\}$ $(i = 1, \dots, n)$が与えられているとする.

ロジスティック回帰では, 総入力にシグモイド関数を適用した値をその入力がクラス$1$に所属する確率と考える.
すなわち, 決定関数$\phi$を
\begin{equation}
    \phi(z) = \frac{1}{1 + \exp(-z)}
\end{equation}
で定義し, 
\begin{equation}
    P(y = 1|\bm{x}, \bm{w}) = \phi(\bm{w}^{T}\bm{x})
\end{equation}
と解釈する.
特に, 分類のみに関心がある場合は, 予測値を
\begin{equation}
    \hat{y} = 
    \begin{cases}
        1 & (\phi(\bm{w}^{T}\bm{x}) \geq 0.5) \\
        0 & (\phi(\bm{w}^{T}\bm{x}) < 0.5)
    \end{cases}
\end{equation}
とする.
重み$\bm{w}$はトレーニングデータが正しく分類される確率の最尤推定として求められる.
すなわち, 尤度
\begin{equation}
    \left\{
        \begin{aligned}
            L(\bm{w}) &= \prod_{i = 1}^{n} \phi(z^{(i)})^{y^{(i)}}(1 - \phi(z^{(i)}))^{1 - y^{(i)}} \\
            z^{(i)} &= \bm{w}^{T}\bm{x}^{(i)}
        \end{aligned}
    \right.
\end{equation}
を最大化する重み$\bm{w}$を探索する.
ただし, 理論・実装の面から対数尤度を最小化する方が簡単であり, 通常はコスト関数
\begin{equation}
    J(\bm{w}) = -\log L(\bm{w}) = -\sum_{i = 1}^{n} \left[y^{(i)}\log \phi(z^{(i)}) + (1 - y^{(i)})\log(1 - \phi(z^{(i)}))\right]
\end{equation}
を最小化する重み$\bm{w}$を探索する.
\begin{align}
    \frac{\partial \phi}{\partial w_{j}}(z)
    &= -\frac{\exp(-z)}{(1 + \exp(-z))^{2}}x_{j} \\
    &= -(1 - \phi(z))\phi(z)x_{j}
\end{align}
に注意すると, 
\begin{align}
    \frac{\partial J}{\partial w_{j}}(\bm{w})
    &= \sum_{i = 1}^{n} \left[y^{(i)}(1 - \phi(z^{(i)})) - (1 - y^{(i)})\phi(z^{(i)})\right]x^{(i)}_{j} \\
    &= \sum_{i = 1}^{n} (y^{(i)} - \phi(z^{(i)}))x^{(i)}_{j}
\end{align}
となるので, ロジスティック回帰における重みの更新規則はADALINEと同じ形式になる.
ロジスティック回帰は線型分離可能な分類問題に対して高い性能を発揮する.

過学習(モデルの汎化性能が低下する現象)を避けるため, コスト関数を正則化することがある.
すなわち, 極端な重みにペナルティを課すことでモデルの複雑さを緩和する.
例えば, $L^{2}$正則化では, 元のコスト関数に正則化項を追加した
\begin{equation}
    \tilde{J}(\bm{w}) = J(\bm{w}) + \frac{1}{2}\lambda\sum_{j = 1}^{m} w_{j}^{2}
\end{equation}
を新たなコスト関数とする\footnote{正則化項にバイアスユニット$w_{0}$を含めることもある.}.
ここで, $\lambda$は正則化パラメータであり, 正則化の強さを制御する.
なお, 後述するSVMの慣例に従い, scikit-learnの実装では逆正則化パラメータ
\begin{equation}
    C = \frac{1}{\lambda}
\end{equation}
を制御している.

\subsection{1対他の手法}
二値分類のアルゴリズムは多値分類のアルゴリズムに拡張することができる.
すなわち, 各クラスについて自身とそれ以外のクラスに分類する二値分類器を構築し, それらの分類器のうち最大の確率を返すクラスラベルを予測値とすればよい.
このような手法を1対他(OvR)の手法と呼ぶ.

具体例として, ロジスティック回帰にOvRを適用してみる.
いま, 与えられた入力を$k$個のクラス$1, \dots, k$に分類する多値分類タスクを考える.
各クラスを自身とそれ以外に分類する関数を
\begin{equation}
    r_{\nu}(y) = 
    \begin{cases}
        1 & (y = \nu) \\
        -1 & (y \neq \nu)
    \end{cases}
    (\nu = 1, \dots, k)
\end{equation}
で定義する.
各$\nu$について$\bm{x}^{(i)}$を入力値, $r_{\nu}(y^{(i)})$を出力値とするロジスティック回帰を実行し,
そこで得られた重みを$\bm{w}_{\nu}$とする.
OvRでは, 最終的な分類器の予測値を
\begin{equation}
    \hat{y} = \arg \max_{\alpha} \phi(\bm{w}_{\alpha}^{T}\bm{x})
\end{equation}
とする.

\subsection{サポートベクトルマシン}
与えられた入力をクラス$1$(陽性クラス)とクラス$-1$(陰性クラス)に分類する二値分類タスクを考える.
トレーニングデータとして, 入力値$\bm{x}^{(i)} \in \R^{m}$と対応するクラス$y^{(i)} \in \{-1, 1\}$ $(i = 1, \dots, n)$が与えられているとする.
サポートベクトルマシン(SVM)はマージンを最大化することにより2値分類を行う.
いま, データセットを超平面
\begin{equation}
    w_{0} + \bm{w}^{T}\bm{x} = 0
\end{equation}
で分離することを考える.
この超平面に対して2つの超平面
\begin{align}
    w_{0} + \bm{w}^{T}\bm{x} &= 1 \\
    w_{0} + \bm{w}^{T}\bm{x} &= -1
\end{align}
の距離をマージンと呼ぶ.
マージンは$\bm{w}$の関数として
\begin{equation}
    \frac{2}{\norm{\bm{w}}}
\end{equation}
と表される.
SVMでは, 正負の超平面のそれぞれの内側に陽性クラスと陰性クラスが含まれるような重み$\bm{w}$のうち, マージンが最大になるものを探索する.
すなわち, 
\begin{equation}
    \left\{
        \begin{aligned}
            & \min_{\bm{w}} \frac{1}{2}\norm{\bm{w}}^{2} \\
            & y^{(i)}(w_{0} + \bm{w}^{T}\bm{x}^{(i)}) \geq 1 \quad (i = 1, \dots, n)
        \end{aligned}
    \right.
\end{equation}
という最適化問題に帰着される.
この定式化は誤分類を許容しないことからハードマージンと呼ばれる.
これに対して, 誤分類を許容する定式化はソフトマージンと呼ばれる.
具体的には, 線型制約を緩和するスラック変数$\xi^{(1)}, \dots, \xi^{(n)}$を導入し, 
\begin{equation}
    \left\{
        \begin{aligned}
            & \min_{\bm{w}} \frac{1}{2}\norm{\bm{w}}^{2} + C\sum_{i = 1}^{n} \xi^{(i)} \\
            & y^{(i)}(w_{0} + \bm{w}^{T}\bm{x}^{(i)}) \geq 1 - \xi^{(i)} \quad (i = 1, \dots, n) \\
            & \xi^{(i)} \geq 0 \quad (i = 1, \dots, n) \\
            & \xi^{(i)} \geq 1 - y^{(i)}(w_{0} + \bm{w}^{T}\bm{x}^{(i)}) \quad (i = 1, \dots, n)
        \end{aligned}
    \right.
\end{equation}
という最適化問題を考える.
コスト関数に追加された項$\sum_{i = 1}^{n} \xi^{(i)}$は誤分類に対するペナルティであり\footnote{$\sum_{i = 1}^{n} \xi^{(i)} \leq K$のとき, 誤分類したサンプルは$K$個以下であることが示される.}, $C$はそれを制御する正則化パラメータである.

SVMは制約付き最適化問題である.
そこで, Lagrange関数
\begin{equation}
    L(\bm{w}, \bm{\xi}, \bm{\alpha}, \bm{\mu}) = \frac{1}{2}\norm{w}^{2} + C\sum_{i = 1}^{n} \xi^{(i)} - \sum_{i = 1}^{n} \alpha^{(i)}(y^{(i)}(w_{0} + \bm{w}^{T}\bm{x}^{(i)}) - 1 + \xi^{(i)}) - \sum_{i = 1}^{n} \mu^{(i)}\xi^{(i)}
\end{equation}
を導入し, 双対問題
\begin{equation}
    \max_{\bm{\alpha} \geq \bm{0}, \bm{\mu} \geq \bm{0}} \min_{\bm{w}, \bm{\xi}} L(\bm{w}, \bm{\xi}, \bm{\alpha}, \bm{\mu})
\end{equation}
を考える.
Lagrange関数$L$は主変数$\bm{w}$, $\bm{\xi}$に関して凸関数であるから, 最適値$\bm{w}^{*}$, $\bm{\xi}^{*}$において
\begin{align}
    0 &= \frac{\partial L}{\partial w_{0}}L(\bm{w}^{*}, \bm{\xi}^{*}, \bm{\alpha}, \bm{\mu}) = -\sum_{i = 1}^{n} \alpha^{(i)}y^{(i)} \\
    \bm{0} &= \frac{\partial L}{\partial \bm{w}}L(\bm{w}^{*}, \bm{\xi}^{*}, \bm{\alpha}, \bm{\mu}) = \bm{w} - \sum_{i = 1}^{n} \alpha^{(i)}y^{(i)}\bm{x}^{(i)} \\
    \bm{0} &= \frac{\partial L}{\partial \bm{\xi}}L(\bm{w}^{*}, \bm{\xi}^{*}, \bm{\alpha}, \bm{\mu}) = C\bm{1} - \bm{\alpha} - \bm{\mu}
\end{align}
が成り立つ.
したがって, 双対問題は
\begin{equation}
    \left\{
        \begin{aligned}
            & \min_{\bm{\alpha}} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \frac{1}{2}\alpha^{(i)}\alpha^{(j)}y^{(i)}y^{(j)}\bm{x}^{(i)T}\bm{x}^{(j)} - \sum_{i = 1}^{n} \alpha^{(i)} \\
            & \sum_{i = 1}^{n} \alpha^{(i)}y^{(i)} = 0 \\
            & 0 \leq \alpha^{(i)} \leq C \quad (i = 1, \dots, n)
        \end{aligned}
    \right.
\end{equation}
という最適化問題に帰着される.
最終的なコスト関数では入力値の内積$\bm{x}^{(i)T}\bm{x}^{(j)}$のみ現れる.
これに注目すると, SVMを非線型分類問題に拡張することができる.
いま, 入力値の変換関数$\bm{\phi} \colon \R^{m} \to \R^{m'}$が与えられたとして, 超曲面
\begin{equation}
    w_{0} + \bm{w}^{T}\bm{\phi}(\bm{x}) = 0
\end{equation}
に関するSVMを考える.
これは, $\bm{\phi}(\bm{x}^{(1)}), \dots, \bm{\phi}(\bm{x}^{(n)})$を入力値とするSVMにほかならない.
このとき, 
\begin{equation}
    K(\bm{x}^{(i)}, \bm{x}^{(j)}) = \bm{\phi}(\bm{x}^{(i)})^{T}\bm{\phi}(\bm{x}^{(j)})
\end{equation}
を満たす関数$K$が存在したとすると, 上の双対問題は
\begin{equation}
    \left\{
        \begin{aligned}
            \min_{\bm{\alpha}} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \frac{1}{2}\alpha^{(i)}\alpha^{(j)}y^{(i)}y^{(j)}K(\bm{x}^{(i)}, \bm{x}^{(j)}) - \sum_{i = 1}^{n} \alpha^{(i)} \\
            \sum_{i = 1}^{n} \alpha^{(i)}y^{(i)} = 0 \\
            0 \leq \alpha^{(i)} \leq C \quad (i = 1, \dots, n)
        \end{aligned}
    \right.
\end{equation}
と読み変えられる.
特に, 変換$\bm{\phi}(\bm{x}^{(i)})$を陽に計算する必要がないことに注意する.
このような関数$K$をカーネル関数といい, カーネル関数を利用したSVMをカーネルSVMという.
代表的なカーネル関数としてRBFカーネル
\begin{equation}
    K(\bm{x}^{(i)}, \bm{x}^{(j)}) = \exp\left(-\gamma\norm{\bm{x}^{(i)} - \bm{x}^{(j)}}^{2}\right)
\end{equation}
が挙げられる.
ただし, $\gamma > 0$はハイパーパラメータである.

\subsection{決定木学習}
決定木学習アルゴリズムでは, 情報利得が最大となる特徴量でデータを分割することを繰り返す.
親ノードのデータセット$D_{p}$を子ノードのデータセット$D_{1}, \dots, D_{m}$に分割するときの情報利得は
\begin{equation}
    G(D_{p}) = I(D_{p}) - \sum_{j = 1}^{m} \frac{|D_{j}|}{|D_{p}|}I(D_{j})
\end{equation}
で定義される.
ここで, 関数$I$はデータセットの不純度を与える関数である.
つまり, 情報利得とは親ノードの不純度と子ノードの不純度の重み付き合計の差である.
ほとんどの機械学習ライブラリでは二分決定木, すなわち$m = 2$の場合の決定木を実装している.

以下では, データセット$D$におけるクラス$i \in \{1, \dots, c\}$のサンプルの割合を$p(i|D)$で表す.
二分決定木で利用される主要な不純度は以下の3つである.
\begin{itemize}
    \item
    エントロピー$I_{H}$は
    \begin{equation}
        I_{H}(D) = -\sum_{i = 1}^{c} p(i|D)\log_{2}p(i|D)
    \end{equation}
    で定義される.
    ただし, $0\log_{2}0 = 0$と考える.

    \item
    ジニ不純度$I_{G}$は
    \begin{equation}
        I_{G}(D) = \sum_{i = 1}^{c} p(i|D)(1 - p(i|D)) = 1 - \sum_{i = 1}^{c} p(i|D)^{2}
    \end{equation}
    で定義される.

    \item
    分類誤差$I_{E}$は
    \begin{equation}
        I_{E}(D) = 1 - \max_{1 \leq i \leq c} |p(i|D)|
    \end{equation}
    で定義される.
\end{itemize}

ランダムフォレストは決定木アルゴリズムを利用したアンサンブル学習である.
\begin{algorithm}
    (ランダムフォレスト)
    \begin{enumerate}
        \item
        トレーニングデータセットから$n$個のサンプル(ブートストラップ標本)をランダムに抽出する.

        \item
        以下の手順によりブートストラップ標本から決定木を成長させる.
        \begin{enumerate}
            \item
            $d$個の特徴量をランダムに非復元抽出する.

            \item
            情報利得を最大化することによりノードを分割する.
        \end{enumerate}

    \item
    (1)(2)の手順を$k$回繰り返す.

    \item
    $k$個の決定木の予測から多数決に基づいてクラスラベルを推測する.
\end{enumerate}
\end{algorithm}

アンサンブルモデルは個々の決定木のノイズに対する耐性が高いため, 決定木の個数$k$が十分大きい限りハイパーパラメータ($n$および$d$)をチューニングする必要性は低い.
また, ブートストラップ標本のサイズ$n$はバイアスとバリアンスのトレードオフに基づいて決定される.
すなわち, $n$が小さければ特定のトレーニングサンプルがブートストラップ標本に含まれる確率が低くなるため, 過学習の影響を抑えることができる.
ただし, この場合は決定木アルゴリズムに利用するサンプル数が小さいため, 学習不足に陥る傾向がある.

\subsection{$k$近傍法}
$k$近傍法分類器(KNN)では, 分類したいデータ点の近傍にあるトレーニングデータセットを探索し, それらの多数決によってクラスラベルを決定する.
\begin{algorithm}
    (KNN)
    \begin{enumerate}
        \item
        探索する近傍のデータ点の個数$k$と距離指標を選択する.

        \item
        分類したいデータ点に最も近い$k$個のサンプルデータ点を探索する.

        \item
        サンプルデータ点の多数決に基づいてクラスラベルを決定する.
    \end{enumerate}
\end{algorithm}
KNNのでは新たなサンプルに対して分類器をすぐに適応させることができる.
一方, 最悪の場合, 新たなサンプルを分類するための計算量はトレーニングデータセットのサイズに比例する.
また, トレーニングデータを破棄することができないため, データセットが大きい場合には記憶域が不足するという実装上の問題が現れる.

近傍点の個数$k$は過学習と学習不足のトレードオフから決定される.
特に, KNNは次元の呪いの影響を受けて過学習に陥りやすいことに注意する.
また, データセットの特徴量に応じて距離指標を選択する必要がある.
実数値のデータセットの場合, Euclid距離を選択することが一般的である.


\section{データ前処理ーよりよいトレーニングセットの構築}
\subsection{欠測データへの対処}
欠測データへの対処法としては以下のようなものが考えられる.
\begin{itemize}
    \item
    欠測値を持つサンプル・特徴量を取り除く.
    例えば, pandasのDataFrameオブジェクトのdropnaメソッドにより実装される.
    \item
    欠測値を補完する.
    例えば、scikit-learnのSimpleImputerオブジェクトのtransformメソッドにより実装される\footnote{
        scikit-learnの主要なオブジェクトは以下に分類される.
        \begin{itemize}
            \item
            推定器:データから学習するfitメソッドを実装している.
            \item
            予測器:予測を実行するpredictメソッドを実装している.
            \item
            変換器:データを変換するtransformメソッドを実装している.
        \end{itemize}
    }.
\end{itemize}

\subsection{カテゴリデータの処理}
カテゴリデータは名義特徴量と順序特徴量に分類される.
順序特徴量とは, 順序付け可能なカテゴリ値である.
例えば, Tシャツのサイズは順序特徴量である.
名義特徴量とは, 順序付け可能でないカテゴリ値である.
例えば, Tシャツの色は名義特徴量である.

通常, 機械学習のモデルを適用するためにはカテゴリデータを整数値に変換する必要がある.
ここで, 名義特徴量を整数値に変換するにあたり, 名義特徴量の各カテゴリ値を整数値に変換することは不適切である.
なぜなら, この変換により本来は存在しない順序関係が発生するからである.
この問題に対処するため, one-hotエンコーディングという手法が用いられる.
これは, 名義特徴量の各カテゴリ値に対してダミー特徴量を導入するというものである.
すなわち, $k$種類のカテゴリ$c_{1}, \dots, c_{k}$がある場合, カテゴリ$c_{i}$を$\R^{k}$の$i$番目の標準基底$\bm{e}_{i}$に変換する.

one-hotエンコーディングを使用する場合, 多重共線性の問題が発生する可能性がある.
つまり, 特徴量の相関性が高い場合, 逆行列の計算が数値的に不安定になる恐れがある.
この問題を解決するためには, 特徴量の列を一つ削除すればよい.
特徴量の列を一つ削除しても元の情報が失われないことに注意する.

\subsection{データセットをトレーニングデータセットとテストデータセットに分割する}
データセットの分割にはトレードオフを伴う.
すなわち, トレーニングデータセットを大きくすると, 学習モデルはデータセットからより多くの情報を得られるが, 汎化誤差の推定値が正確でなくなる.
慣習的には, トレーニングデータセットとテストデータセットの比率を$6:4$, $7:3$, $8:2$にすることが多い.
ただし, データセットが十分大きい場合には$9:1$や$99:1$という比率にすることもある.

\subsection{特徴量の尺度を揃える}
多くの最適化アルゴリズムは, 複数の特徴量の尺度が同じである場合にうまく動作する.
特徴量のスケーリング手法としては正規化と標準化が一般的である.
正規化とは特徴量を区間$[0, 1]$の範囲にスケーリングし直すことである.
これは, min-maxスケーリング
\begin{equation}
    x_{\text{norm}}^{(i)} = \frac{x^{(i)} - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}
\end{equation}
において$x_{\text{min}} = 0$, $x_{\text{max}} = 1$としたものである.
一方, 標準化とは特徴量の平均が$0$, 分散が$1$となるようにスケーリングし直すことである.
すなわち, 
\begin{align}
    x_{\text{std}}^{(i)} &= \frac{x^{(i)} - \mu_{\bm{x}}}{\sigma_{\bm{x}}} \\
    \mu_{\bm{x}} &= \frac{1}{n}\sum_{i = 1}^{n} x^{(i)} \\
    \sigma_{\bm{x}}^{2} &= \frac{1}{n}\sum_{i = 1}^{n} (x^{(i)} - \mu_{\bm{x}})^{2}
\end{align}
という変換である.
標準化には外れ値に対する情報が失われないという特徴がある.

\subsection{有益な特徴量の選択}
汎化誤差を減らすための方法には以下のようなものがある.
\begin{itemize}
    \item
    トレーニングデータをさらに集める.
    \item
    正則化により複雑さに対してペナルティを課す.
    \item
    パラメータの個数が少ない単純なモデルを選択する.
    \item
    データの次元を減らす.
\end{itemize}
実用上, より多くのトレーニングデータを集めることは難しい.
ここでは, 正則化と特徴選択による次元削減の手法を考えることにする.

\subsubsection{$L^{1}$正則化と$L^{2}$正則化}
コスト関数$J(\bm{w})$が与えられているとする.
このとき, $L^{1}$正則化では
\begin{equation}
    \tilde{J}(\bm{w}) = J(\bm{w}) + \norm{w}_{1}, \quad \norm{w}_{1} = \sum_{i = 1}^{n} |w_{i}|
\end{equation}
を新たなコスト関数として最適化問題を解く.
$L^{1}$正則化はスパース性, すなわち多くの要素が$0$となるような重み$\bm{w}$を導く.
一方, $L^{2}$正則化では
\begin{equation}
    \tilde{J}(\bm{w}) = J(\bm{w}) + \norm{w}_{2}^{2}, \quad \norm{w}_{2} = \left(\sum_{i = 1}^{n} w_{i}^{2}\right)^{1/2}
\end{equation}
を新たなコスト関数として最適化問題を解く.
$L^{2}$正則化は重みベクトル$\bm{w}$を$\bm{0}$に近づける効果をもたらす.

\subsubsection{逐次特徴選択アルゴリズム}
逐次後退選択(SBS)アルゴリズムは, 分類器の性能に与える影響が最も小さい特徴量を逐次的に取り除くことで汎化誤差の削減を目指す.
\begin{algorithm}
    (SBS)
    特徴量の添え字集合を$S = \{1, \dots, m\}$, 目標の次元を$k \leq m$とする.
    $|S| = k$となるまで, 以下の手順を繰り返す.
    \begin{enumerate}
        \item
        次に削除する添え字$\mu$を
        \begin{equation}
            \left\{
                \begin{aligned}
                    \mu = \arg\max_{j \in S} J(\bm{w}; r_{j}(\bm{x}^{(1)}), \dots, r_{j}(\bm{x}^{(m)})) \\
                    r_{j}(\bm{x}) = (x_{1}, \dots, x_{j - 1}, x_{j + 1}, \dots, x_{m})
                \end{aligned}
            \right.
        \end{equation}
        により決定する.
        \item
        添え字集合$S$を
        \begin{equation}
            S \leftarrow S \setminus \{j\}
        \end{equation}
        により更新する.
    \end{enumerate}
\end{algorithm}

SBSアルゴリズムのように, 逐次的に特徴量を選択していくアルゴリズムを一般に逐次特徴選択アルゴリズムと呼ぶ.


\section{次元削減でデータを圧縮する}
\subsection{主成分分析による教師なし次元削減}
主成分分析(PCA)は特徴抽出と次元削減のための線型変換である.
PCAの目的は最大の分散方向を含む部分空間に高次元データを射影することである.
\begin{algorithm}
    (PCA)
    データセット$\bm{x}^{(1)}, \dots, \bm{x}^{(n)} \in \R^{d}$と特徴空間の次元$k \leq d$が与えられたとする.
    \begin{enumerate}
        \item
        データセットを標準化し, それを改めて$\bm{x}^{(1)}, \dots, \bm{x}^{(n)}$とする.

        \item
        共分散行列$\bm{\Sigma}$を
        \begin{equation}
            \left\{
            \begin{aligned}
                &\sigma_{ij} = \frac{1}{n}\sum_{k = 1}^{n} (x^{(k)}_{i} - \mu_{i})(x^{(k)}_{j} - \mu_{j}) \\
                &\bm{\mu} = \frac{1}{n}\sum_{k = 1}^{n} \bm{x}^{(k)}
            \end{aligned}
            \right.
        \end{equation}
        により計算する.

        \item
        共分散行列を固有値分解する.
        \begin{equation}
            \left\{
            \begin{aligned}
                &\bm{\Sigma} = \bm{V}^{T}\bm{\Lambda}\bm{V} \\
                &\bm{V} = (\bm{v}_{1}, \dots, \bm{v}_{d}), \quad \bm{V}^{T}\bm{V} = \bm{I} \\
                &\bm{\Lambda} = \mathrm{diag}(\lambda_{1}, \dots, \lambda_{d}) \quad (\lambda_{1} \geq \dots \geq \lambda_{d})
            \end{aligned}
            \right.
        \end{equation}

        \item
        部分空間にデータ点を射影する.
        \begin{equation}
            \left\{
            \begin{aligned}
                &\bm{x}' = \bm{W}^{T}\bm{x} \\
                &\bm{W} = (\bm{v}_{1}, \dots, \bm{v}_{k})
            \end{aligned}
            \right.
        \end{equation}
    \end{enumerate}
\end{algorithm}

共分散行列の固有ベクトル$\bm{v}_{1}, \dots, \bm{v}_{d}$を主成分という.
また, 固有値全体の和に対する特定の固有値の比率$\lambda_{j}/\sum_{i = 1}^{d} \lambda_{i}$を分散説明率という.
大きな固有値を持つ主成分が張る部分空間にデータ点を射影することにより, そのデータ点が持つ情報を保ちながら次元を削減することを目指す.

\subsection{線型判別分析による教師ありデータ圧縮}
線型判別分析(LDA)は学習アルゴリズムとしても次元削減の手段としても用いられる.
LDAの目的はクラスを最もよく分離する部分空間に高次元データを射影することである.
\begin{algorithm}
    (LDA)
    データセット$\bm{x}^{(1)}, \dots, \bm{x}^{(n)} \in \R^{d}$, 対応するクラスラベル$y^{(1)}, \dots, y^{(n)} \in \{1, \dots, c\}$および特徴空間の次元$k \leq d$が与えられたとする.
    \begin{enumerate}
        \item
        データセットを標準化し, それを改めて$\bm{x}^{(1)}, \dots, \bm{x}^{(n)}$とする.

        \item
        クラス間変動行列$\bm{S}_{B}$およびクラス内変動行列$\bm{S}_{W}$を
        \begin{align}
            \bm{S}_{B} &= \sum_{l = 1}^{c}n_{l}(\bm{m}_{l} - \bm{m})(\bm{m}_{l} - \bm{m})^{T} \\
            \bm{S}_{W} &= \sum_{l = 1}^{c}\sum_{y^{(i)} = l}\frac{1}{n_{l}}(\bm{x}^{(i)} - \bm{m}_{i})(\bm{x}^{(i)} - \bm{m}_{i})^{T}
        \end{align}
        により計算する.
        ただし, 
        \begin{align}
            n_{l} &= |\{i | y^{(i)} = l\}| \\
            \bm{m}_{l} &= \frac{1}{n_{l}}\sum_{y^{(i)} = l} \bm{x}^{(i)} \\
            \bm{m} &= \frac{1}{n}\sum_{i = 1}^{n} \bm{x}^{(i)}
        \end{align}
        である.

        \item
        行列$\bm{S}_{W}^{-1}\bm{S}_{B}$を固有値分解する.
        \begin{equation}
            \left\{
            \begin{aligned}
                &\bm{S}_{W}^{-1}\bm{S}_{B} = \bm{V}^{T}\bm{\Lambda}\bm{V} \\
                &\bm{V} = (\bm{v}_{1}, \dots, \bm{v}_{d}), \quad \bm{V}^{T}\bm{V} = \bm{I} \\
                &\bm{\Lambda} = \mathrm{diag}(\lambda_{1}, \dots, \lambda_{d}) \quad (\lambda_{1} \geq \dots \geq \lambda_{d})
            \end{aligned}
            \right.
        \end{equation}

        \item
        部分空間にデータ点を射影する.
        \begin{equation}
            \left\{
            \begin{aligned}
                &\bm{x}' = \bm{W}^{T}\bm{x} \\
                &\bm{W} = (\bm{v}_{1}, \dots, \bm{v}_{k})
            \end{aligned}
            \right.
        \end{equation}
    \end{enumerate}
\end{algorithm}

クラス間変動行列$\bm{S}_{B}$は階数が$1$以下の行列の和であるから, $0$でない固有値は高々$c - 1$個である.
つまり, LDAは最大で$c$個のクラスを線型分離することができる.
LDAを次元削減の手法として利用する場合は, 新しい特徴空間で機械学習アルゴリズムを実行すればよい.

\subsection{カーネル主成分分析を使った非線形写像}
カーネルPCA(KPCA)はカーネルトリックによりPCAを非線形問題に拡張したものである.
いま, 特徴空間への変換関数$\phi$が与えられたとする.
特徴空間におけるデータセットの平均を
\begin{equation}
    \bm{\xi} = \frac{1}{n}\sum_{i = 1}^{n} \phi(\bm{x}^{(i)})
\end{equation}
とすると, 特徴空間における共分散行列$\bm{\Sigma}$は
\begin{equation}
    \bm{\Sigma} = \frac{1}{n}\sum_{i = 1}^{n}(\phi(\bm{x}^{(i)}) - \bm{\xi})(\phi(\bm{x}^{(i)}) - \bm{\xi})^{T}
\end{equation}
と表される.
したがって, $\lambda$を$\bm{\Sigma}$の固有値, $\bm{v}$を$\lambda$に対する$\bm{\Sigma}$の固有ベクトルとすると
\begin{equation}
    \lambda\bm{v} = \bm{\Sigma}\bm{v} = \frac{1}{n}\sum_{i = 1}^{n}(\phi(\bm{x}^{(i)}) - \bm{\xi})(\phi(\bm{x}^{(i)}) - \bm{\xi})^{T}\bm{v}
\end{equation}
すなわち, 
\begin{align}
    \bm{v} &= \frac{1}{n\lambda}\sum_{i = 1}^{n}a^{(i)}(\phi(\bm{x}^{(i)}) - \bm{\xi}) \\
    a^{(i)} &= (\phi(\bm{x}^{(i)}) - \bm{\xi})^{T}\bm{v}
\end{align}
が成り立つ.
ここで, 
\begin{align}
    \phi(\bm{X}) &= (\phi(\bm{x}^{(1)}), \dots, \phi(\bm{x}^{(n)}))^{T} \\
    \bm{\Xi} &= (\bm{\xi}, \dots, \bm{\xi})^{T}
\end{align}
とおくと
\begin{align}
    \bm{v} &= \frac{1}{n\lambda}(\phi(\bm{X}) - \bm{\Xi})^{T}\bm{a} \\
    \bm{a} &= (\phi(\bm{X}) - \bm{\Xi})\bm{v}
\end{align}
であるから, 
\begin{equation}
    \bm{L} = (\phi(\bm{X}) - \bm{\Xi})(\phi(\bm{X}) - \bm{\Xi})^{T}
\end{equation}
として
\begin{align*}
    n\lambda\bm{L}\bm{a}
    &= (\phi(\bm{X}) - \bm{\Xi})(\phi(\bm{X}) - \bm{\Xi})^{T}(\phi(\bm{X}) - \bm{\Xi})\bm{v} \\
    &= \phi(\bm{X}) - \bm{\Xi})(\phi(\bm{X}) - \bm{\Xi})^{T}(\phi(\bm{X}) - \bm{\Xi})(\phi(\bm{X}) - \bm{\Xi})^{T}\bm{a} \\
    &= \bm{L}^{2}\bm{a}
\end{align*}
が成り立つ.
$L$は対称行列であり, それゆえ特徴空間全体を張る固有ベクトル集合を持つので
\begin{equation}
    \bm{L}\bm{a} = n\lambda\bm{a}
\end{equation}
を得る.
つまり, $\bm{a}$は$\bm{L}$の固有ベクトルであり, これから特徴空間における共分散行列$\bm{\Sigma}$の固有ベクトルが求まる.
\begin{align*}
    L_{ij}
    &= (\phi(\bm{x}^{(i)}) - \bm{\xi})^{T}(\phi(\bm{x}^{(j)}) - \bm{\xi}) \\
    &= \phi(\bm{x}^{(i)})^{T}\phi(\bm{x}^{(j)}) - \frac{1}{n}\sum_{k = 1}^{n}\phi(\bm{x}^{(k)})^{T}\phi(\bm{x}^{(j)}) - \frac{1}{n}\sum_{l = 1}^{n}\phi(\bm{x}^{(i)})^{T}\phi(\bm{x}^{(l)}) + \frac{1}{n^{2}}\sum_{k = 1}^{n} \sum_{l = 1}^{n}\phi(\bm{x}^{(k)})^{T}\phi(\bm{x}^{(l)})
\end{align*}
であるから, カーネル行列$\bm{K}$を
\begin{equation}
    K_{ij} = \phi(\bm{x}^{(i)})^{T}\phi(\bm{x}^{(j)})
\end{equation}
により定義すると, 
\begin{equation}
    \bm{L} = \bm{K} - \bm{1}_{n}\bm{K} - \bm{K}\bm{1}_{n} + \bm{1}_{n}\bm{K}\bm{1}_{n}
\end{equation}
を得る.
カーネルトリック, すなわち
\begin{equation}
    \mathcal{K}(\bm{x}, \bm{y}) = \phi(\bm{x})^{T}\phi(\bm{y})
\end{equation}
を満たすカーネル関数$\mathcal{K}$を利用すると, 変換関数$\phi$を明示的に計算することなくカーネル行列$\bm{K}$(および中心化行列$\bm{L}$)が求まる.
KPCAでは, 特徴空間における共分散行列ではなくカーネル行列を固有値分解する.
\begin{algorithm}
    (KPCA)
    データセット$\bm{x}^{(1)}, \dots, \bm{x}^{(n)} \in \R^{d}$と特徴空間の次元$k \leq d$が与えられたとする.
    \begin{enumerate}
        \item
        中心化行列$\bm{L}$を
        \begin{equation}
            \bm{L} = \bm{K} - \bm{1}_{n}\bm{K} - \bm{K}\bm{1}_{n} + \bm{1}_{n}\bm{K}\bm{1}_{n}
        \end{equation}
        により計算する.

        \item
        中心化行列を固有値分解する.
        \begin{equation}
            \left\{
            \begin{aligned}
                &\bm{L} = \bm{A}^{T}\bm{\Theta}\bm{A} \\
                &\bm{A} = (\bm{a}^{(1)}, \dots, \bm{a}^{(n)}), \quad \bm{A}^{T}\bm{A} = \bm{I} \\
                &\bm{\Theta} = \mathrm{diag}(\theta_{1}, \dots \theta_{n}) \quad (\theta_{1} \geq \dots \geq \theta_{n})
            \end{aligned}
            \right.
        \end{equation}

        \item
        部分空間にデータ点を射影する.
        \begin{equation}
            \left\{
            \begin{aligned}
                &\bm{x}' = \bm{W}^{T}\phi(\bm{x}) \\
                &\bm{W} = (\bm{v}_{1}, \dots, \bm{v}_{k}) \\
                &\bm{v}_{i} = \frac{1}{\theta_{i}}(\phi(\bm{X}) - \bm{\Xi})^{T}\bm{a}^{(i)}
            \end{aligned}
            \right.
        \end{equation}
    \end{enumerate}

部分空間への射影は変換関数$\phi$を明示的に計算することなく求まる.
実際, 
\begin{align*}
    x'_{i}
    &= \bm{v}_{i}^{T}\phi(\bm{x}) \\
    &= \frac{1}{\theta_{i}}\bm{a}^{(i)T}(\phi(\bm{X}) - \bm{\Xi})\phi(\bm{x}) \\
    &= \frac{1}{\theta_{i}}\sum_{j = 1}^{n} a^{(i)}_{j}\left[\mathcal{K}(\bm{x}^{(j)}, \bm{x}) - \frac{1}{n}\sum_{k = 1}^{n} \mathcal{K}(\bm{x}^{(k)}, \bm{x})\right]
\end{align*}
となるので, カーネル行列を計算できればよい.
\end{algorithm}


\section{モデルの評価とハイパーパラメータのチューニングのベストプラクティス}
\subsection{$k$分割交差検証を使ったモデルの性能の評価}
機械学習の汎化性能を評価する代表的な手法として, ホールドアウト法と$k$分割交差検証がある.

\subsubsection{ホールドアウト法}
ホールドアウト法では, 与えられたデータセットを以下の3つに分割する.
\begin{itemize}
    \item
    トレーニングデータセット：機械学習アルゴリズムを実行するために用いる.

    \item
    検証データセット：学習により得られた機械学習アルゴリズムの性能を評価するために用いる.

    \item
    テストデータセット：最終的な機械学習アルゴリズムの性能を評価するために用いる.
\end{itemize}
様々なハイパーパラメータに対してトレーニングデータセットによる学習と検証データセットによる性能評価を繰り返す.
ハイパーパラメータのチューニングを終えたら, テストデータセットにより最終的な性能を評価する.
ホールドアウト法には, トレーニングデータセットと検証データセットの分割の仕方が性能の評価に影響を及ぼすという問題がある.

\subsubsection{$k$分割交差検証法}
$k$分割交差検証法では, 与えられたデータセットをトレーニングデータセットとテストデータセットに分割したあと, テストデータセットをさらに$k$個のサブセットに分割する.
このうち, $k - 1$個のサブセットを機械学習アルゴリズムの学習に利用し, 残りの$1$個を性能評価に利用する.
性能評価に使用するサブセットを変えることで$k$個のモデルを取得し, それらの平均を現在のハイパーパラメータでの性能指標とする.
ハイパーパラメータを変えながらこの手順を繰り返し, チューニングを終えたらトレーニングデータセット全体を利用して機械学習アルゴリズムを実行する.
こうして得られた機械学習アルゴリズムの性能をテストデータセットにより評価する.

$k$分割交差検証法では, 学習結果の検証に利用するデータセットが複数存在する.
このため, ホールドアウト法と比較すると, 性能評価においてトレーニングデータセットと検証データセットの分割の仕方の影響を受けにくい.
特に, 元のデータセットのクラスの比率を保ちながらトレーニングデータセットを$k$個のサブセットに分割する手法を層化$k$分割交差検証法と呼ぶ.

\subsection{学習曲線と検証曲線によるアルゴリズムの診断}
学習不足と過学習の問題に対処する代表的なツールとして学習曲線と検証曲線がある.
\begin{itemize}
    \item
    学習曲線は, トレーニングサンプルのサイズの関数として正解率を表した曲線である.
    バイアスが高いモデルでは, サンプルサイズを増やしてもトレーニングデータセットとテストデータセットの正解率が低い.
    一方, バリアンスが高いモデルでは, サンプルサイズを増やしてもトレーニングデータセットとテストデータセットの間で正解率の差が大きい.

    \item
    検証曲線は, ハイパーパラメータの関数として正解率を表した曲線である.
    バイアスが高い点では, トレーニングデータセット・テストデータセットのいずれに対しても正解率が低い.
    一方, バリアンスが高い点では, トレーニングデータセットとテストデータセットの間で正解率の差が大きい.
\end{itemize}

\subsection{グリッドサーチによる機械学習モデルのチューニング}
最適なハイパーパラメータを見出す手法として, 検証曲線の他にグリッドサーチも広く用いられている.
これは, 複数のハイパーパラメータの組み合わせすべてに対して学習を行い, 最も正解率が高いモデルを採用するというものである.
このため, グリッドサーチによるハイパーパラメータのチューニングは計算量が非常に大きい.

\subsection{さまざまな性能評価指標}
モデルの妥当性を評価する指標には, 正解率の他に適合率, 再現率, F1スコアなどがある.

\subsubsection{混同行列を解釈する}
混同行列は, 真陽性, 真陰性, 偽陽性, 偽陰性からなる正方行列である.
すなわち, 予測値が真陽性, 真陰性, 偽陽性, 偽陰性となるデータ点の個数をそれぞれ$N_{TP}$, $N_{TN}$, $N_{FP}$, $N_{FN}$で表すと, 混同行列$M_{c}$は
\begin{equation}
    M_{c} = \begin{pmatrix}
        N_{TP} & N_{FN} \\
        N_{FP} & N_{TN}
    \end{pmatrix}
\end{equation}
で与えられる\footnote{この行列の転置を混同行列と定義することもある.}.

\subsubsection{分類モデルの適合率と再現率を最適化する}
予測の誤分類率$R_{\mathrm{err}}$と正解率$R_{\mathrm{acc}}$はそれぞれ
\begin{align}
    R_{\mathrm{err}} &= \frac{N_{FP} + N_{FN}}{N_{TP} + N_{TN} + N_{FP} + N_{FN}} \\
    R_{\mathrm{acc}} &= \frac{N_{TP} + N_{TN}}{N_{TP} + N_{TN} + N_{FP} + N_{FN}} = 1 - R_{\mathrm{err}}
\end{align}
で定義される.
また, 真陽性率$R_{\mathrm{tpr}}$と偽陽性率$R_{\mathrm{fpr}}$はそれぞれ
\begin{align}
    R_{\mathrm{tpr}} &= \frac{N_{TP}}{N_{TP} + N_{FN}} \\
    R_{\mathrm{fpr}} &= \frac{N_{FP}}{N_{TN} + N_{FP}}
\end{align}
で定義される.
さらに, 適合率$R_{\mathrm{pre}}$と再現率$R_{\mathrm{rec}}$は
\begin{align}
    R_{\mathrm{pre}} &= \frac{N_{TP}}{N_{TP} + N_{FP}} \\
    R_{\mathrm{rec}} &= \frac{N_{TP}}{N_{TP} + N_{FN}} = R_{\mathrm{tpr}}
\end{align}
で定義される.
つまり, 適合率は陽性と予測したデータ点のうち真陽性であるデータ点の割合を表し, 再現率は実際に陽性であるデータ点のうち真陽性であるデータ点の割合を表す.
実際の性能評価では, 適合率と再現率を組み合わせた性能指標であるF1スコアが広く利用されている.
これは, 
\begin{equation}
    F_{1} = \frac{2R_{\mathrm{pre}}R_{\mathrm{rec}}}{R_{\mathrm{pre}} + R_{\mathrm{rec}}}
\end{equation}
で与えられる.

\subsubsection{ROC曲線をプロットする}
受信者操作特性(ROC)曲線は, $x$軸に偽陽性率, $y$軸に真陽性率をとり, モデルの予測値に対する閾値を媒介変数として性能指標をプロットした曲線である.
また, ROC曲線と$x$軸で囲まれた部分の面積を曲線下面積(AUC)と呼ぶ.
AUCは分類モデルの性能を表ていると考えることができる.
特に, 完全な推定器ではROC曲線が直線$y = 1$に一致する.

例として, ロジスティック回帰による二値分類タスクを考える. 
パラメータ$0 \leq t \leq 1$に対して決定関数$\phi$による予測値を
\begin{equation}
    \hat{y} =
    \begin{cases}
        1 &\quad (\phi(z) \geq t) \\
        0 &\quad (\phi(z) < t)
    \end{cases}
\end{equation}
で定義し, この閾値による分類のもとでの真陽性率を$R_{\mathrm{tpr}}(t)$, 偽陽性率を$R_{\mathrm{fpr}}(t)$とする.
ROC曲線は, $t$を媒介変数とする曲線$(R_{\mathrm{fpr}}(t), R_{\mathrm{tpr}}(t))$である.
ROC曲線やAUCを計算するためには, データセットの各データ点に対して一度だけ予測を行えば十分であることに注意する.

\subsubsection{多クラス分類のための性能指標}
多値分類タスクのための代表的な性能指標はマイクロ平均とマクロ平均である.
クラス$1, \dots, k$に対する予測値が真陽性となるデータ点の個数を$N_{TP, 1}, \dots, N_{TP, k}$, 偽陽性となるデータ点の個数を$N_{FP, 1}, \dots, N_{FP, k}$とすると, マイクロ平均は
\begin{equation}
    R_{\mathrm{micro}} = \frac{\sum_{\mu = 1}^{k} N_{TP, \mu}}{\sum_{\mu = 1}^{k} (N_{TP, \mu} + N_{FP, \mu})}
\end{equation}
で与えられる.
また, クラス$1, \dots, k$に対する適合率を$R_{\mathrm{pre}, 1}, \dots, R_{\mathrm{pre}, k}$とすると, マクロ平均は
\begin{equation}
    R_{\mathrm{macro}} = \frac{1}{k}\sum_{\mu = 1}^{k} R_{\mathrm{pre, \mu}}
\end{equation}
で与えられる.

\subsection{クラスの不均衡に対処する}
クラスラベルの個数に偏りがある場合, 適切な性能指標を選択することが重要になる.
例えば, 多数派クラスが90\%を占めるデータセットの場合, 単純に多数派クラスを予測するだけで90\%の正解率を達成できる.
このような場合には, 適合率や再現率など正解率以外の性能指標を用いるべきである.

また, 性能指標だけではなく機械学習アルゴリズム自体もクラスの不均衡に影響される.
つまり, コスト関数はトレーニングサンプルに対する合計値として算出されるため, 多数派クラスが学習結果に寄与しやすい.
このような不均衡に対処する一般的な手法は存在せず, 問題に応じて適切な解決策を見出す必要がある.
例えば, 以下のような手法が広く利用されている.
\begin{itemize}
    \item
    少数派クラスの予測を誤った場合に大きなペナルティを課す.

    \item
    少数派クラスのアップサンプリングまたは多数派クラスのダウンサンプリングを行う.

    \item
    人工的なトレーニングデータを生成する.
\end{itemize}


\section{回帰分析}
\subsection{線型回帰}
回帰モデルとは, 連続値の目的変数を予測するモデルである.
特に, 説明変数の線型関数として目的変数を表す回帰を線型回帰と呼ぶ.

\subsection{Housingデータセットの探索}
機械学習モデルを構築する前に探索的データ解析を行うことが推奨される.
これは, 散布図, ヒストグラム, 相関行列などからデータの相関関係や外れ値を見出す作業である.

\subsection{最小二乗線型回帰モデルの実装}
トレーニングデータとして, 入力値$\bm{x}^{(i)} \in \R^{m}$と対応するクラス$y^{(i)} \in \R$ $(i = 1, \dots, n)$が与えられているとする.
最小二乗線型回帰モデルでは, コスト関数として誤差平方和
\begin{equation}
    J(\bm{w}) = \frac{1}{2}\sum_{i = 1}^{n} (y^{(i)} - \bm{w}^{T}\bm{x})^{2}
\end{equation}
を利用し, 勾配降下法により最適な重み$\bm{w}$を探索する.

\subsection{RANSACを使ったロバスト回帰モデルの学習}
線型回帰モデルは外れ値の影響を受けやすい.
RANSAC(RANdom SAmple Cosensus)アルゴリズムは回帰モデルに正常値(外れ値でないデータ点からなるサブセット)を学習させる.
\begin{algorithm}
    (RANSAC)
    \begin{enumerate}
        \item
        ランダムな個数のデータ点を正常値として選択し, モデルを学習させる.

        \item
        (1)のモデルでその他のデータ点を評価し, 許容範囲に収まるデータ点を正常値に追加する.

        \item
        すべての正常値を利用してモデルを学習させる.

        \item
        学習済みモデルの誤差を推定する.

        \item
        誤差が閾値より小さいまたは繰り返し回数の上限に到達した場合, アルゴリズムを終了する.
        そうでない場合, (1)に戻る.
    \end{enumerate}
\end{algorithm}

\subsection{線型回帰モデルの性能評価}
分類モデルと同じように, 回帰モデルでもトレーニングデータセットで学習を行い, テストデータセットでその性能を評価することが重要である.
いま, 目的変数の真値を$y^{(i)}$, 回帰モデルによる予測値を$\hat{y}^{(i)}$ $(i = 1, \dots, n)$とする.
線型回帰モデルの代表的な性能指標には次のようなものがある.
\begin{itemize}
    \item
    残差：テストデータの予測値と真値の差を計算する.
    \begin{equation}
        E_{r}^{(i)} = y^{(i)} - \hat{y}^{(i)}
    \end{equation}
    残差プロットに規則性が現れる場合, 回帰モデルはデータセットの情報の一部を捉えられていない.

    \item
    平均二乗誤差：テストデータの統計的分散を計算する.
    \begin{equation}
        E_{\mathrm{MSE}} = \frac{1}{n}\sum_{i = 1}^{n} (y^{(i)} - \hat{y}^{(i)})^{2}
    \end{equation}

    \item
    決定係数：平均二乗誤差を目的変数の真値で標準化する.
    \begin{equation}
        \left\{
        \begin{aligned}
            &R^{2} = 1 - \frac{E_{\mathrm{MSE}}}{\sigma_{y}} \\
            &\sigma_{y} = \frac{1}{n}\sum_{i = 1}^{n} (y^{(i)} - \mu_{y})^{2} \\
            &\mu_{y} = \frac{1}{n}\sum_{i = 1}^{n} y^{(i)}
        \end{aligned}
        \right.
    \end{equation}
\end{itemize}

\subsection{回帰に正則化手法を適用する}
線型回帰に対する代表的な正則化手法は以下の3つである.
\begin{itemize}
    \item
    リッジ回帰：コスト関数に$L^{2}$ペナルティを課す.
    \begin{equation}
        J(\bm{w}) = \frac{1}{2}\sum_{i = 1}^{n} (y^{(i)} - \bm{w}^{T}\bm{x}^{(i)})^{2} + \lambda\sum_{k = 1}^{m} |w_{j}|^{2}
    \end{equation}

    \item
    LASSO(Least Absolute Shrinkage and Selection Operator：コスト関数に$L^{1}$ペナルティを課す.
    \begin{equation}
        J(\bm{w}) = \frac{1}{2}\sum_{i = 1}^{n} (y^{(i)} - \bm{w}^{T}\bm{x}^{(i)})^{2} + \lambda\sum_{k = 1}^{m} |w_{j}|
    \end{equation}

    \item
    Elastic Net法：コスト関数に$L^{1}$ペナルティおよび$L^{2}$ペナルティを課す.
    \begin{equation}
        J(\bm{w}) = \frac{1}{2}\sum_{i = 1}^{n} (y^{(i)} - \bm{w}^{T}\bm{x}^{(i)})^{2} + \lambda_{1}\sum_{k = 1}^{m} |w_{j}| + \lambda_{2}\sum_{k = 1}^{m} |w_{j}|^{2}
    \end{equation}
\end{itemize}

\subsection{多項式回帰：線型回帰モデルから曲線を見い出す}
説明変数と目的変数の関係が非線形の場合, 単純な線型回帰はうまく機能しない.
この問題に対処する一つの方法は, 説明変数の高次の項を追加したうえで線型回帰を適用するというものである.
つまり,
\begin{equation}
    J(\bm{w}) = \sum_{d = 0}^{m} \sum_{|I_{d}| = 0}w_{d, I_{d}}\bm{x}^{I_{d}}
\end{equation}
というコスト関数に対して線型回帰を適用する.
ただし, $I_{d}$は長さ$d$の多重指数, $\bm{x} = (\bm{x}^{(1)}, \dots, \bm{x}^{(n)})$である.

多項式の次数が大きくなるほどモデルが複雑になるため, 過学習に陥りやすい.
また, 多項式は非線形関係を表現する唯一の方法ではなく, 指数関数など別の関数形を仮定して回帰を適用するべき場合もある.

\subsection{ランダムフォレストを使って非線形関係に対処する}
決定木は回帰問題に適用することができる.
この場合, ノードの不純度として二乗平均誤差
\begin{equation}
    \left\{
    \begin{aligned}
        &I(D) = \frac{1}{|D|}\sum_{i \in D} (y^{(i)} - \mu_{D}) \\
        &\mu_{D} = \frac{1}{|D|}\sum_{i \in D} y^{(i)}
    \end{aligned}
    \right.
\end{equation}
を利用し, 情報利得が最小となるように決定木を成長させる.

ランダムフォレスト回帰は, このような決定木に対してランダムフォレストアルゴリズムを適用したアンサンブル回帰である.
すなわち, ランダムフォレスト分類に現れる決定木の情報利得として二乗平均誤差を利用すればよい.


\section{クラスタ分析ーラベルなしデータの分析}
本章では教師なし学習の一種であるクラスタ分析を説明する.
クラスタ分析の目的は, クラスラベルが事前に分からないデータセットを適切にグループ分けすることである.
クラスタ分析は以下の3つ分類される.
\begin{itemize}
    \item
    プロトタイプベースクラスタリング
    \item
    階層的クラスタリング
    \item
    密度ベースクラスタリング
\end{itemize}

\subsection{k-means法を使った類似度によるオブジェクトのグループ化}
\subsubsection{scikit-learnを使ったk-meansクラスタリング}
k-means法はプロトタイプベースクラスタリングである.
\begin{algorithm}
    (k-means法)
    \begin{enumerate}
        \item
        データセットから$k$個のセントロイド$\bm{\mu}^{(1)}, \dots, \bm{\mu}^{(k)}$をランダムに選択する.

        \item
        データセットの各点を最も近いセントロイドに割り当てる.

        \item
        セントロイドに割り当てられたサンプル点の中心にセントロイドを移動する.
        すなわち, セントロイド$\mu^{(j)}$に割り当てられたサンプル点の集合を$D_{j}$とすると, 新しいセントロイドは
        \begin{equation}
            \bm{\mu}^{(j)} = \frac{1}{|D_{j}|}\sum_{\bm{x^{(i)}} \in D_{j}} \bm{x}^{(i)}
        \end{equation}
        で与えられる.

        \item
        サンプル点へのクラスタの割り当てが変化しなかった場合, または事前に設定した閾値・繰り返し回数に到達した場合, アルゴリズムを終了する.
        そうでない場合, (2)に戻る.
    \end{enumerate}
\end{algorithm}

サンプル点の類似度の指標としてEuclid距離を用いた場合, k-means法のコスト関数はクラスタ内誤差平方和(クラスタの慣性)
\begin{equation}
    \left\{
    \begin{aligned}
        &J = \sum_{i = 1}^{n} \sum_{j = 1}^{k} w^{(i, j)}\norm{\bm{x}^{(i)} - \bm{\mu}^{(j)}}_{2}^{2} \\
        &\bm{\mu}^{(j)} = \frac{1}{|D_{j}|}\sum_{\bm{x^{(i)}} \in D_{j}} \bm{x}^{(i)} \\
    \end{aligned}
    \right.
\end{equation}
で与えられる.
ただし, $D_{j}$は$\bm{\mu}^{(j)}$をセントロイドとするクラスタに属するサンプル点の集合であり, 
\begin{equation}
    w^{(i, j)} = 
    \begin{cases}
        0 &\quad (\bm{x}^{(i)} \notin D_{j}) \\
        1 &\quad (\bm{x}^{(i)} \in D_{j})
    \end{cases}
\end{equation}
とする.

単純なk-means法にはいくつか問題点がある.
\begin{itemize}
    \item
    クラスタの個数$k$を指定しなければならない.
    実際の問題では, 事前にクラスタの個数が分かることはない.

    \item
    クラスタが階層的でない(オーバラップしない).
    つまり, データセットがクラスタごとに分離されている必要がある.

    \item
    各クラスタにデータ点が少なくとも一つ含まれることを前提としている.
    実装上は, クラスタが空になった場合にセントロイドの割り当てを変更する必要がある.
\end{itemize}

\subsubsection{k-means++法を使ってセントロイドの初期値をよりスマートに設定する}
セントロイドの初期値によっては適切にクラスタリングされなかったり, 収束が遅くなったりすることがある.
このため, 通常は異なるセントロイドの初期値からk-means法のアルゴリズムを実行し, コスト関数が最小となったクラスタリングを採用する.
また, k-means++法によりセントロイドの初期値を適切に選択することが推奨される.
この方法では, セントロイドの初期値が互いに離れて配置される確率が高くなる.

\begin{algorithm}
    (k-means++法)
    \begin{enumerate}
        \item
        データセットからランダムにデータ点を選択し, 集合$M$に格納する.

        \item
        $|M| = k$となるまで以下の手順を繰り返す.

        \begin{enumerate}
            \item
            データ点$\bm{x}^{(i)} \notin M$と$M$の距離$d^{(i)}$を計算する.
            すなわち, 
            \begin{equation}
                d^{(i)} = \min_{\bm{\mu} \in M} \norm{\bm{x}^{(i)} - \bm{\mu}}_{2}^{2}
            \end{equation}
            とする.

            \item
            データ点$\bm{x}^{(i)} \notin M$が選ばれる確率が
            \begin{equation}
                \frac{(d^{(i)})^{2}}{\sum_{\bm{x}^{(j)} \notin M} (d^{(j)})^{2}}
            \end{equation}
            となる確率分布に従ってデータ点をランダムに選択し, 集合$M$に追加する.
        \end{enumerate}

        \item
        $M$の各点ををセントロイドの初期値としてk-means法によるクラスタリングを実行する.
    \end{enumerate}
\end{algorithm}

\subsubsection{ハードクラスタリングとソフトクラスタリング}
ハードクラスタリングではそれぞれのデータ点にただ一つのラベルを割り当てる.
k-means法はハードクラスタリングの一種である.
これに対して, ソフトクラスタリングではデータ点に1つ以上のラベルを割り当てる.
ソフトクラスタリングの代表的なアルゴリズムとしてFuzzy C-means(FCM)法がある.

\begin{algorithm}
    (FCM法)
    \begin{enumerate}
        \item
        セントロイドの個数$k$とファジー係数$m \geq 1$を指定する.
        また, 各データ点にクラスタ所属確率をランダムに割り当てる.

        \item
        クラスタのセントロイドを
        \begin{equation}
            \bm{\mu}^{(j)} = \frac{\sum_{i = 1}^{n} (w^{(i, j)})^{m}\bm{x}^{(i)}}{\sum_{i = 1}^{n} (w^{(i, j)})^{m}}
        \end{equation}
        により計算する.

        \item
        各データ点のクラスタ所属確率を
        \begin{equation}
            w^{(i, j)} = \left[\sum_{\alpha = 1}^{k} \left(\frac{\norm{\bm{x}^{(i)} - \bm{\mu}^{(j)}}_{2}}{\norm{\bm{x}^{(i)} - \bm{\mu}^{(\alpha)}}_{2}}\right)^{\frac{2}{m - 1}}\right]^{-1}
        \end{equation}
        により更新する.

        \item
        クラスタ所属確率が変化しなかった場合, または事前に設定した閾値・繰り返し回数に到達した場合, アルゴリズムを終了する.
        そうでない場合, (2)に戻る.
    \end{enumerate}
\end{algorithm}

FCM法のコスト関数は
\begin{equation}
    J = \sum_{i = 1}^{n} \sum_{j = 1}^{k} (w^{(i, j)})^{m}\norm{\bm{x}^{(i)} - \bm{\mu}^{(j)}}_{2}^{2}
\end{equation}
で与えられる.
ファジー係数$m$が大きいほどクラスタ所属確率は小さくなり, より曖昧なクラスタリングを許容するようになる.

\subsubsection{エルボー法を使ってクラスタの最適な個数を求める}
k-means法でクラスタの最適な個数を求める方法の一つにエルボー法がある.
エルボー法では, クラスタ慣性の増加率が最大になる$k$をクラスタの個数として採用する.
つまり, クラスタの個数が$k$の場合のクラスタ慣性を$I_{k}$で表すと, $I_{k} - I_{k - 1}$が最小となる$k$をクラスタの個数とする.

\subsubsection{シルエット図を使ってクラスタリングの性能を数値化する}
シルエット分析はシルエット係数と呼ばれる数値によりクラスタリングの性能を評価する.
いま, サンプルデータ$\bm{x}^{(1)}, \dots, \bm{x}^{(n)}$がクラスタ$C_{1}, \dots, C_{k}$に分類されたとする.
このとき, データ点$\bm{x}^{(i)} \in C_{j}$に対するシルエット係数$s^{(i)}$は
\begin{equation}
    \left\{
    \begin{aligned}
        s^{(i)} &= \frac{b^{(i)} - a^{(i)}}{\max\{a^{(i)}, b^{(i)}\}} \\
        a^{(i)} &= \frac{1}{|C_{j}| - 1} \sum_{\alpha \neq i, \bm{x}^{(\alpha)} \in C_{j}} \norm{\bm{x}^{(i)} - \bm{x}^{(\alpha)}} \\
        b^{(i)} &= \min_{\beta \neq j} \frac{1}{|C_{\beta}|}\sum_{\bm{x}^{(\alpha) \in C_{\beta}}} \norm{\bm{x}^{(\alpha)} - \bm{x}^{(\beta)}}
    \end{aligned}
    \right.
\end{equation}
で与えられる.
$a^{(i)}$はクラスタ$C_{j}$に関する凝集度を表し, $b^{(i)}$は最も近いクラスタに関する乖離度を表す.
また, シルエット係数を棒グラフで表現した図をシルエット図という.

理想的なクラスタリングでは, 各データ点について$b^{(i)} \gg a^{(i)}$, したがって$s^{(i)} \simeq 1$となり, シルエット図は1付近で平坦になる.
シルエット係数の計算方法から分かるように, シルエット分析はk-means法だけではなく他のクラスタリングアルゴリズムにも適用することができる.


\subsection{クラスタを階層木として構成する}
階層的クラスタリングでは樹形図によりクラスタリングの結果を表現することができる.
階層的クラスタリングには凝集型と分割型の2種類の方法がある.
\begin{enumerate}
    \item
    分割型階層的クラスタリングはトップダウン方式のクラスタリングアルゴリズムである.
    この方法では, 最初にすべてのサンプルを含むクラスタを定義し, 各クラスタがサンプルを一つだけ含む状態になるまでクラスタを分割していく.

    \item
    凝集型階層的クラスタリングはボトムアップ方式のクラスタリングアルゴリズムである.
    この方法では, 最初に各サンプルを一つのクラスタとして定義し, クラスタが一つになる状態までクラスタを統合していく.
\end{enumerate}
以下では, 凝集型クラスタリングアルゴリズムを考察する.

\subsubsection{ボトムアップ方式でのクラスタのグループ化}
凝集型クラスタリングアルゴリズムの手続きは以下のようにまとめられる.
\begin{algorithm}
    (凝集型クラスタリング)
    \begin{enumerate}
        \item
        それぞれのデータ点を単一のクラスタとみなし, クラスタ間の距離行列を計算する.

        \item
        最も距離が近いクラスタを統合する.

        \item
        クラスタ間の距離行列を更新する.

        \item
        クラスタが1つになった場合, アルゴリズムを終了する.
        そうでない場合, (2)に戻る.
    \end{enumerate}
\end{algorithm}

データ点の距離が与えられた場合, クラスタ間の距離の計算方法には以下のようなものがある.
\begin{enumerate}
    \item
    単連結法：最も近いサンプル点の距離をクラスタ間の距離とする.
    \begin{equation}
        d(C, D) = \min_{\bm{x} \in C, \bm{y} \in D} \norm{\bm{x} - \bm{y}}
    \end{equation}

    完全連結法：最も遠いサンプル点の距離をクラスタ間の距離とする.
    \begin{equation}
        d(C, D) = \max_{\bm{x} \in C, \bm{y} \in D} \norm{\bm{x} - \bm{y}}
    \end{equation}

    重心法：クラスタの重心の距離をクラスタ間の距離とする.
    \begin{equation}
        d(C, D) = \norm{\frac{1}{|C|}\sum_{\bm{x} \in C} \bm{x} - \frac{1}{|D|}\sum_{\bm{y} \in D} \bm{y}}
    \end{equation}
\end{enumerate}

\subsubsection{距離行列で階層的クラスタリングを実行する}
scipyではpdist関数によりデータ点の距離行列を計算することができる.
また, linkage関数によりデータ点の距離行列から連結行列を計算することができる.
ここで, 連結行列とはクラスタの統合過程を表現した行列である.
また, 樹形図は連結行列をグラフで表現したものである.

\subsubsection{樹形図をヒートマップと組み合わせる}
階層的クラスタリングの樹形図はヒートマップと組み合わせて図示することが一般的である.
つまり, 各データ点の特徴量をヒートマップとして行列で表現し, その隣にクラスタリングの過程を樹形図で表現する.

\subsubsection{scikit-learnを使って凝集型階層的クラスタリングを適用する}
scikit-learnではAgglomerativeClusteringクラスにより凝集型階層的クラスタリングを実行することができる.
この場合, クラスタの個数を指定してクラスタリングを実行するため, 統合の過程を見るためには異なるクラスタの個数を指定してクラスタリングを実行する必要がある.


\subsection{DBSCANを使って高密度の領域を特定する}
DBSCAN(Density-Based Spatial Clustering of Applications with Noise)は密度ベースクラスタリングの一種である.
\begin{algorithm}
    (DBSCAN)
    \begin{enumerate}
        \item
        探索半径$\varepsilon$と探索個数$M$を指定する.

        \item
        データ点を以下の3種類に分類する.
        \begin{itemize}
            \item
            コア点：そのデータ点から半径$\varepsilon$以内に他のデータ点が$M$個以上存在するデータ点.

            \item
            ボーダ点：コア点でないデータ点のうち, そのデータ点から半径$\varepsilon$以内にコア点が存在するデータ点.

            \item
            ノイズ点：コア点でもボーダ点でもないデータ点.
        \end{itemize}

        \item
        距離が$\varepsilon$以下のコア点を接続し, クラスタを形成する.

        \item
        ボーダ点を対応するコア点のクラスタに割り当てる.
    \end{enumerate}
\end{algorithm}

k-means法と異なり, DBSCANではクラスタが球状に分布する必要はない.
また, すべてのデータ点をクラスタに割り当てるとは限らず, ノイズを除去することができる.


\section{数学的理論}
\subsection{カーネル法}
以下, 本節では$\mathcal{X}$を空でない集合とする.
\begin{definition}
    写像$k \colon \mathcal{X} \times \mathcal{X} \to \R$が$\mathcal{X}$上のカーネル関数であるとは, 実Hilbert空間$\mathcal{H}$と関数$\phi \colon \mathcal{X} \to \mathcal{H}$が存在し, 任意の$x, y \in \mathcal{X}$に対して
    \begin{equation}
        k(x, y) = \langle\phi(x), \phi(y)\rangle_{\mathcal{H}}
    \end{equation}
    が成り立つことをいう.
    このとき, $\mathcal{H}$を$k$の特徴空間, $\phi$を$k$の特徴写像という.
\end{definition}
カーネル関数は半正定値対称関数であることに注意する.
実際, 任意の$a_{1}, \dots, a_{n} \in \R$, $x_{1}, \dots, x_{n} \in \mathcal{X}$に対して
\begin{align*}
    \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{i}a_{j}k(x_{i}, x_{j})
    &= \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{i}a_{j}\langle\phi(x_{i}), \phi(x_{j})\rangle_{\mathcal{H}} \\
    &= \left\langle\sum_{i = 1}^{n} a_{i}\phi(x_{i}), \sum_{j = 1}^{n} a_{j}\phi(x_{j})\right\rangle_{\mathcal{H}} \\
    &= \norm{\sum_{i = 1}^{n} a_{i}\phi(x_{i})}^{2} \\
    &\geq 0
\end{align*}
となる.
実は, この主張の逆も成り立つ.
\begin{theorem}
    任意の半正定値対称関数$k \colon \mathcal{X} \times \mathcal{X} \to \R$はカーネル関数である.
\end{theorem}

この結果により様々なカーネル関数が得られる.
\begin{lemma}
    $k$, $l$を$\mathcal{X}$上のカーネル関数, $\alpha > 0$とする.
    このとき, $\alpha k$, $k + l$は$\mathcal{X}$上のカーネル関数である.
\end{lemma}
\begin{lemma}
    $A \colon \mathcal{X} \to \mathcal{Y}$を写像, $l$を$\mathcal{Y}$上のカーネル関数とする.
    このとき, 写像$k \colon \mathcal{X} \times \mathcal{X} \to \R$を
    \begin{equation}
        k(x, y) := l(A(x), A(y))
    \end{equation}
    で定義すると, $k$は$\mathcal{X}$上のカーネル関数である.
\end{lemma}
\begin{lemma}
    $k$, $l$をそれぞれ$\mathcal{X}$, $\mathcal{Y}$上のカーネル関数とする.
    このとき, 写像$k \times l$は$\mathcal{X} \times \mathcal{Y}$上のカーネル関数である.
    特に, $\mathcal{X} = \mathcal{Y}$のとき, 写像$kl$は$\mathcal{X}$上のカーネル関数である.
\end{lemma}
\begin{corollary}
    $m \in \N$, $c \geq 0$とする.
    このとき, 関数$k \colon \R^{d} \times \R^{d} \to \R$を
    \begin{equation}
        k(x, y) = \left(\langle x, y\rangle + c)\right)^{m}
    \end{equation}
    で定義すると, $k$は$\R^{d}$上のカーネル関数である.
    これを多項式カーネル関数という.
\end{corollary}
\begin{lemma}
    関数$f \colon \R \to \R$が
    \begin{equation}
        f(z) = \sum_{n = 0}^{\infty} a_{n}z^{n}, \quad a_{n} \geq 0, |z| < r, r > 0
    \end{equation}
    という形にTaylor展開されるとする.
    このとき, $\mathcal{X} = \{x \in \R^{d} | \norm{x} < \sqrt{r}\}$として, 関数$k \colon \mathcal{X} \times \mathcal{X} \to \R$を
    \begin{equation}
        k(x, y) = f(\langle x, y \rangle)
    \end{equation}
    で定義すると, $k$は$\mathcal{X}$上のカーネル関数である.
\end{lemma}
\begin{corollary}
    関数$k \colon \R^{d} \times \R^{d} \to \R$を
    \begin{equation}
        k(x, y) = \exp\left(\langle x, y\rangle\right)
    \end{equation}
    で定義すると, $k$は$\R^{d}$上のカーネル関数である.
    これを指数カーネル関数という.
\end{corollary}
\begin{corollary}
    関数$k \colon \R^{d} \times \R^{d} \to \R$を
    \begin{equation}
        k(x, y) = \exp\left(-\gamma\norm{x - y}^{2}\right)
    \end{equation}
    で定義すると, $k$は$\R^{d}$上のカーネル関数である.
    これをGaussカーネル関数または動径基底関数という.
\end{corollary}

\begin{definition}
    $\mathcal{H}$を$\mathcal{X}$上の実数値関数からなる実Hilbert空間とする.
    写像$k \colon \mathcal{X} \times \mathcal{X} \to \mathcal{H}$が以下の条件を満たすとき, $k$を$\mathcal{H}$上の再生カーネル関数といい, $\mathcal{H}$を再生核Hilbert空間という.
    \begin{enumerate}
        \item
        $\forall \, x \in \mathcal{X} \quad k(\cdot, x) \in \mathcal{H}$
        \item
        $\forall \, x \in \mathcal{X} \quad \forall \, f \in \mathcal{H} \quad \langle f, k(\cdot, x) \rangle_{\mathcal{H}} = f(x)$
    \end{enumerate}
\end{definition}

\begin{theorem}
    (Moore-Aronszajnの定理)
    $\mathcal{X}$上のカーネル関数$k$に対して, 再生核Hilbert空間が一意に存在し, $k$が$\mathcal{H}$上の再生カーネル関数になる.
\end{theorem}

\begin{thebibliography}{9}
    \bibitem{Sebastian}
    [第2版] Python機械学習プログラミング 達人データサイエンティストによる理論と実践, Sebastian Raschka, Vahid Mirjalili(著), 福島真太朗(監訳), 2018, インプレス
    \bibitem{Kanamori}
    機械学習のための連続最適化, 金森敬文, 鈴木大慈, 竹内一朗, 佐藤一誠, 2016, 講談社
    \bibitem{Arthur}
    Introduction to RKHS, and some simple kernel algorithms, Arthur Gretton, 2019, \url{http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf}
    \bibitem{Scholkopf}
    Nonlinear Component Analysis as a Kernel Eigenvalue Problem, Sch\"{o}lkopf, Bernhard, 1998, Neural Computation. 10 (5): 1299–1319
\end{thebibliography}
\end{document}
